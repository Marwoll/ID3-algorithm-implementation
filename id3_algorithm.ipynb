{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"mushrooms.csv\")\n",
    "for column in X.columns: \n",
    "        X[column] = pd.factorize(X[column])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, test_size):\n",
    "\n",
    "    \n",
    "    #sets the test size if the parameter is a percentage\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(X))\n",
    "    \n",
    "    #test_indices is picked randomly\n",
    "    indices = X.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_set = X.loc[test_indices]\n",
    "    train_set = X.drop(test_indices)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same numbers of data will appear every time\n",
    "random.seed(0)\n",
    "#gets training_set and test_set of the dataframe X\n",
    "train_X, test_X = train_test_split(X, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets data to a two dimmensional numpy array\n",
    "data = train_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in X.values and checks on its labels, returns true or false if there is only one class\n",
    "def check_purity(data):\n",
    "    \"\"\"\n",
    "    Title: \n",
    "        check_purity\n",
    "    \n",
    "    Description: \n",
    "        Check if a dataset only contains one label\n",
    "    \n",
    "    Parameters:\n",
    "        data(array): a dataset of the dataframe\n",
    "    \n",
    "    Returns:\n",
    "        True/False: depending on the length of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    y = data[:, 0]\n",
    "    unique_classes = np.unique(y)\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \"\"\"\n",
    "    Title: \n",
    "        classify_data\n",
    "    \n",
    "    Description: \n",
    "        Classifies the data. \n",
    "    \n",
    "    Parameters:\n",
    "        data(array): a dataset of the dataframe\n",
    "    \n",
    "    Returns:\n",
    "        classification(int): classification of the dataset. 0 or 1\n",
    "    \"\"\"\n",
    "    y = data[:, 0]\n",
    "    unique_classes, counts_unique_classes = np.unique(y, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title: \n",
    "        get_potential_splits\n",
    "    \n",
    "    Description: \n",
    "        Splits the dataset into a dictionary that contains the \n",
    "        potential splits of a column based on the values\n",
    "    \n",
    "    Parameters:\n",
    "        data(array): a dataset of the dataframe\n",
    "    Returns:\n",
    "        potential_splits(dict) : key = column_index and values is potential split values\n",
    "    \"\"\"\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    \n",
    "    #exclude the label which is the first column\n",
    "    for column_index in range(1, n_columns): \n",
    "        potential_splits[column_index] = []\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        for index in range(len(unique_values)):\n",
    "            if index != 0:\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "                \n",
    "                potential_splits[column_index].append(potential_split)\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        split_data\n",
    "    \n",
    "    Desciption:\n",
    "        splits the dataset in two based on column and value\n",
    "        \n",
    "    Parameters:\n",
    "        data(array): dataset of the dataframe\n",
    "        split_column(int): the column index of best splitting column\n",
    "        split_value(float): the value of where to split at\n",
    "    \n",
    "    Returns\n",
    "        data_below(array): a dataset\n",
    "        data_above(array): a dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        entropy\n",
    "        \n",
    "    Description:\n",
    "        calculates the entropy of a dataset\n",
    "        \n",
    "    Parameters:\n",
    "        data(array): a dataset\n",
    "        \n",
    "    Returns:\n",
    "        entropy(float): entropy of a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    y = data[:, 0]\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    p = counts / counts.sum()\n",
    "    entropy = sum(p * -np.log2(p))\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_entropy_split(data_below, data_above):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        overall_entropy_split\n",
    "        \n",
    "    Description:\n",
    "        Calculates the overall entropy of two datasets\n",
    "        \n",
    "    Parameters:\n",
    "        data_below(array): a dataset\n",
    "        data_above(array): a dataset\n",
    "        \n",
    "    Returns:\n",
    "        overall_entropy(float): the overall entropy of two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * entropy(data_below) \n",
    "                      + p_data_above * entropy(data_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(data):\n",
    " \n",
    "    \n",
    "    y = data[:, 0]\n",
    "    #counts how many times a label occurs, lies the sum of each label in the array counts\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    prob = counts / counts.sum()\n",
    "    gini = 1 - sum(prob**2)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_gini_split(data_below, data_above):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        overall_gini_split\n",
    "        \n",
    "    Description:\n",
    "        calculates the overall gini index of two datasets\n",
    "        \n",
    "    Parameters:\n",
    "        data_below(array): a dataset\n",
    "        data_above(array): a dataset\n",
    "        \n",
    "    Returns:\n",
    "        overall_gini(float): the overall gini index of two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "    \n",
    "    overall_gini = (p_data_below * gini(data_below)\n",
    "                   + p_data_above * gini(data_above))\n",
    "    \n",
    "    return overall_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data, potential_splits, impurity_measure):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        determine_best_split\n",
    "        \n",
    "    Description:\n",
    "        Finds the best split based on the lowest entropy or gini index\n",
    "        \n",
    "    Parameters:\n",
    "        data(array): a dataset\n",
    "        potential_splits(dict): key = column_index and values is potential split values\n",
    "        impurity_measure(str): a string that determines if best split\n",
    "                                is calculated with gini index or entropy\n",
    "        \n",
    "    Returns:\n",
    "        best_split_column(int): index of best column to split at\n",
    "        best_split_value(float): the best value to split the column at\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_entropy = 9999\n",
    "    overall_gini = 9999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "            if (impurity_measure == 'gini'):\n",
    "                current_overall_gini = overall_gini_split(data_below, data_above)\n",
    "                if current_overall_gini <= overall_gini:\n",
    "                    overall_gini = current_overall_gini\n",
    "                    best_split_column = column_index\n",
    "                    best_split_value = value\n",
    "                    \n",
    "            else:\n",
    "                current_overall_entropy = overall_entropy_split(data_below, data_above)\n",
    "                if current_overall_entropy <= overall_entropy:\n",
    "                    overall_entropy = current_overall_entropy\n",
    "                    best_split_column = column_index\n",
    "                    best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X,impurity_measure, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        learn\n",
    "        \n",
    "    Description:\n",
    "        builds the tree recursivley\n",
    "        \n",
    "    Parameters:\n",
    "        X(dataframe): a dataframe read from a csv file\n",
    "        impurity_measure(str): a string that determines if best split\n",
    "                                is calculated with gini index or entropy\n",
    "        counter(int): counts how many times the recursion has happend\n",
    "        min_samples(int): used to check that the length of the data is above or equeal a sertain number\n",
    "        max_depth(int): the maximum depth the tree can have\n",
    "        \n",
    "    Returns:\n",
    "        classification(int): classification of the dataset. 0 or 1\n",
    "        sub_tree(dict): keys = question values = list of answer\n",
    "    \"\"\"\n",
    "    \n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = X.columns\n",
    "        data = X.values\n",
    "    else:\n",
    "        data = X           \n",
    "    \n",
    "    \n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        \n",
    "        return classification\n",
    "\n",
    "    \n",
    "    #start of the recursion\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        #calling the help functions to get split on the best column and value\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits, impurity_measure)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        #making the sub stree and the questions\n",
    "        column_name = COLUMN_HEADERS[split_column]\n",
    "        question = \"{} <= {}\".format(column_name, split_value)\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        #calling the learn method to get answers recursivley\n",
    "        yes_answer = learn(data_below, impurity_measure, counter, min_samples, max_depth)\n",
    "        no_answer = learn(data_above, impurity_measure, counter, min_samples, max_depth)\n",
    "        \n",
    "        #when the answers are the same there is no need for asking the question\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'odor <= 3.5': [{'odor <= 0.5': [0,\n",
      "                                  {'spore-print-color <= 3.5': [1,\n",
      "                                                                {'stalk-root <= 3.0': [{'habitat <= 2.5': [0,\n",
      "                                                                                                           1]},\n",
      "                                                                                       {'gill-size <= 0.5': [0,\n",
      "                                                                                                             1]}]}]}]},\n",
      "                 0]}\n"
     ]
    }
   ],
   "source": [
    "tree = learn(train_X, impurity_measure='entropy', max_depth=5)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class                          0\n",
       "cap-shape                      3\n",
       "cap-surface                    0\n",
       "cap-color                      4\n",
       "bruises                        1\n",
       "odor                           7\n",
       "gill-attachment                0\n",
       "gill-spacing                   0\n",
       "gill-size                      0\n",
       "gill-color                     8\n",
       "stalk-shape                    1\n",
       "stalk-root                     4\n",
       "stalk-surface-above-ring       2\n",
       "stalk-surface-below-ring       3\n",
       "stalk-color-above-ring         0\n",
       "stalk-color-below-ring         0\n",
       "veil-type                      0\n",
       "veil-color                     0\n",
       "ring-number                    0\n",
       "ring-type                      1\n",
       "spore-print-color              4\n",
       "population                     3\n",
       "habitat                        3\n",
       "classification                 0\n",
       "classification_correct      True\n",
       "Name: 6917, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#makes and example of row number 0\n",
    "example = test_X.iloc[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifies an example, a row by running it through the decision tree\n",
    "def classify_example(example, tree):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        classify_example\n",
    "        \n",
    "    Description:\n",
    "        classifies an example, a row by running it through the decision tree recursivley\n",
    "        \n",
    "    Parameters:\n",
    "        example(array): a row in a dataset\n",
    "        tree(dict): keys = question values = list of answer\n",
    "        \n",
    "    Returns:\n",
    "        answer(int): class of the example. 0 or 1\n",
    "        clssify_example: recursion\n",
    "    \"\"\"\n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if example[feature_name] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_example(example, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split column  5\n",
      "split value  3.5\n"
     ]
    }
   ],
   "source": [
    "potential_splits = get_potential_splits(data)\n",
    "split_column, split_value = determine_best_split(data,potential_splits, impurity_measure='entropy')\n",
    "data_below, data_above = split_data(data, split_column, split_value)\n",
    "print('split column ', split_column)\n",
    "print('split value ', split_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_example(example, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X, tree):\n",
    "    \n",
    "    \"\"\"\n",
    "    Title:\n",
    "        score\n",
    "        \n",
    "    Description:\n",
    "        calculates the accuracy of the test set by running it through the tree\n",
    "        \n",
    "    Parameters:\n",
    "        X(dataframe): a dataframe read from a file\n",
    "        tree(dict): keys = question values\n",
    "    Returns:\n",
    "        accuracy(float): accuracy of the test set runned through the tree\n",
    "    \"\"\"\n",
    "\n",
    "    X[\"classification\"] = X.apply(classify_example, axis=1, args=(tree,))\n",
    "    X[\"classification_correct\"] = X[\"classification\"] == X[\"class\"]\n",
    "    \n",
    "    accuracy = X[\"classification_correct\"].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993846153846154"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = score(test_X, tree)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn desicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = pd.read_csv(\"mushrooms.csv\")\n",
    "for column in X.columns: \n",
    "        X[column] = pd.factorize(X[column])[0]\n",
    "\n",
    "all_inputs = X[X.columns[1:]].values\n",
    "all_classes = X['class'].values\n",
    "\n",
    "#calling the train_test_split method and sets train_set = 0.2 of the dataset\n",
    "(train_inputs, test_inputs, train_classes, test_classes) = train_test_split(all_inputs, all_classes, train_size=0.8, test_size=0.2,random_state=1)\n",
    "\n",
    "dtc = DecisionTreeClassifier('entropy')\n",
    "dtc.fit(train_inputs, train_classes)\n",
    "dtc.score(test_inputs, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
